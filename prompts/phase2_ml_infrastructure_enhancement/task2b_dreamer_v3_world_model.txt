# JAXS Development Task: Dreamer V3 World Model

## Project Description

JAXS (Just Another Exploration Substrate) is a differentiable physics and machine learning environment built in Rust with WebGPU. The project aims to create a platform for novelty-driven artificial life through physics simulation and reinforcement learning.

**Key Architecture Principles:**
- **WebGPU-First**: All compute operations target GPU via WGSL shaders
- **Modular Design**: Clean separation between compute, physics, ML, render, and phenotype crates
- **Zero-Copy**: Efficient GPU memory management with bytemuck for data layout
- **Differentiable**: All operations must preserve gradient flow for end-to-end optimization

**Critical Guidelines:**
- Maintain GPU/CPU backend compatibility in the compute crate
- Use `#[repr(C)]` and `bytemuck::Pod` for all GPU data structures
- Follow existing test patterns with comprehensive coverage
- Preserve backwards compatibility for existing APIs
- Document all public interfaces with rustdoc
- Use existing error types and handling patterns

**Testing Requirements:**
- Add unit tests for all new functionality
- Include integration tests for cross-crate features
- Verify GPU/CPU parity for compute operations
- Test edge cases and error conditions
- Maintain performance benchmarks where applicable

## Phase 2: ML Infrastructure Enhancement

**Goal**: Upgrade the ML framework to support modern RL algorithms and GPU acceleration

This phase focuses on enhancing the machine learning capabilities to support advanced algorithms like Dreamer V3 and enable efficient training through GPU acceleration. The current ML framework operates on CPU with basic tensor operations and simple neural networks. This phase will add GPU tensor operations, implement the Dreamer V3 world model architecture, and expand the RL algorithm library to support state-of-the-art methods.

## Task 2B: Dreamer V3 World Model

**Crate**: `crates/ml/`
**Primary Files**: New `src/world_model.rs`, `src/rssm.rs`, environment integration
**New Files**: `src/world_model/encoder.rs`, `src/world_model/decoder.rs`, `src/world_model/dynamics.rs`
**Test Files**: `tests/dreamer_v3.rs`
**Complexity**: High
**Dependencies**: Task 2A (GPU tensor operations)

### Objective

Implement the core Dreamer V3 architecture for model-based reinforcement learning. Dreamer V3 learns a world model that can predict future states and rewards, enabling imagination-based policy optimization. This is a critical component for the JAXS vision of novelty-driven artificial life, as it provides the latent representations needed for behavioral characterization.

### Dreamer V3 Architecture Overview

Dreamer V3 consists of several key components:
1. **Encoder**: Maps observations to latent representations
2. **Recurrent State Space Model (RSSM)**: Temporal dynamics in latent space
3. **Decoder**: Reconstructs observations from latent states
4. **Reward Predictor**: Predicts rewards from latent states
5. **Continue Predictor**: Predicts episode termination

### Implementation Details

1. **Core World Model Structure** in `src/world_model.rs`:

   ```rust
   use crate::tensor::Tensor;
   use crate::nn::{Dense, Conv2D, LSTM, LayerNorm};

   #[derive(Debug)]
   pub struct DreamerV3 {
       pub encoder: ObservationEncoder,
       pub rssm: RecurrentStateSpaceModel,
       pub decoder: ObservationDecoder,
       pub reward_predictor: RewardPredictor,
       pub continue_predictor: ContinuePredictor,
       pub config: DreamerV3Config,
   }

   #[derive(Debug, Clone)]
   pub struct DreamerV3Config {
       pub latent_dim: usize,
       pub hidden_dim: usize,
       pub discrete_dim: usize,
       pub discrete_classes: usize,
       pub sequence_length: usize,
       pub kl_loss_scale: f32,
       pub reconstruction_loss_scale: f32,
       pub reward_loss_scale: f32,
       pub continue_loss_scale: f32,
       pub use_gpu: bool,
   }

   impl DreamerV3 {
       pub fn new(config: DreamerV3Config, observation_shape: &[usize]) -> Result<Self, MLError> {
           let encoder = ObservationEncoder::new(observation_shape, config.latent_dim, config.use_gpu)?;
           let rssm = RecurrentStateSpaceModel::new(&config)?;
           let decoder = ObservationDecoder::new(config.latent_dim, observation_shape, config.use_gpu)?;
           let reward_predictor = RewardPredictor::new(config.latent_dim, config.use_gpu)?;
           let continue_predictor = ContinuePredictor::new(config.latent_dim, config.use_gpu)?;

           Ok(Self {
               encoder,
               rssm,
               decoder,
               reward_predictor,
               continue_predictor,
               config,
           })
       }

       pub fn encode_sequence(&self, observations: &Tensor, actions: &Tensor) -> Result<RSSMState, MLError> {
           // Encode observation sequence into latent states
           let encoded = self.encoder.forward(observations)?;
           let states = self.rssm.observe(encoded, actions)?;
           Ok(states)
       }

       pub fn imagine(&self, initial_state: &RSSMState, actions: &Tensor) -> Result<Imagination, MLError> {
           // Generate imagined trajectories from initial state
           let imagined_states = self.rssm.imagine(initial_state, actions)?;
           let imagined_observations = self.decoder.forward(&imagined_states.features())?;
           let imagined_rewards = self.reward_predictor.forward(&imagined_states.features())?;
           let imagined_continues = self.continue_predictor.forward(&imagined_states.features())?;

           Ok(Imagination {
               states: imagined_states,
               observations: imagined_observations,
               rewards: imagined_rewards,
               continues: imagined_continues,
           })
       }

       pub fn compute_loss(&self, batch: &WorldModelBatch) -> Result<WorldModelLoss, MLError> {
           // Compute all world model losses
           let states = self.encode_sequence(&batch.observations, &batch.actions)?;
           
           // Reconstruction loss
           let reconstructed = self.decoder.forward(&states.features())?;
           let reconstruction_loss = mse_loss(&reconstructed, &batch.observations)?;
           
           // KL divergence loss for regularization
           let kl_loss = self.rssm.kl_loss(&states)?;
           
           // Reward prediction loss
           let predicted_rewards = self.reward_predictor.forward(&states.features())?;
           let reward_loss = mse_loss(&predicted_rewards, &batch.rewards)?;
           
           // Continue prediction loss
           let predicted_continues = self.continue_predictor.forward(&states.features())?;
           let continue_loss = bce_loss(&predicted_continues, &batch.continues)?;
           
           let total_loss = reconstruction_loss * self.config.reconstruction_loss_scale
               + kl_loss * self.config.kl_loss_scale
               + reward_loss * self.config.reward_loss_scale
               + continue_loss * self.config.continue_loss_scale;

           Ok(WorldModelLoss {
               total: total_loss,
               reconstruction: reconstruction_loss,
               kl: kl_loss,
               reward: reward_loss,
               continue: continue_loss,
           })
       }
   }
   ```

2. **Recurrent State Space Model (RSSM)** in `src/rssm.rs`:

   ```rust
   #[derive(Debug)]
   pub struct RecurrentStateSpaceModel {
       pub rnn: LSTM,
       pub discrete_prior: Dense,
       pub discrete_posterior: Dense,
       pub discrete_dim: usize,
       pub discrete_classes: usize,
       pub hidden_dim: usize,
   }

   #[derive(Debug, Clone)]
   pub struct RSSMState {
       pub deterministic: Tensor, // h_t (RNN hidden state)
       pub stochastic: Tensor,    // z_t (discrete latent variables)
       pub logits: Tensor,        // Categorical distribution parameters
   }

   impl RSSMState {
       pub fn features(&self) -> Tensor {
           // Concatenate deterministic and stochastic parts
           Tensor::cat(&[&self.deterministic, &self.stochastic], 1)
       }
   }

   impl RecurrentStateSpaceModel {
       pub fn new(config: &DreamerV3Config) -> Result<Self, MLError> {
           let rnn = LSTM::new(
               config.latent_dim + config.discrete_dim * config.discrete_classes,
               config.hidden_dim,
               config.use_gpu
           )?;
           
           let discrete_prior = Dense::new(config.hidden_dim, config.discrete_dim * config.discrete_classes, config.use_gpu)?;
           let discrete_posterior = Dense::new(config.hidden_dim + config.latent_dim, config.discrete_dim * config.discrete_classes, config.use_gpu)?;

           Ok(Self {
               rnn,
               discrete_prior,
               discrete_posterior,
               discrete_dim: config.discrete_dim,
               discrete_classes: config.discrete_classes,
               hidden_dim: config.hidden_dim,
           })
       }

       pub fn initial_state(&self, batch_size: usize) -> Result<RSSMState, MLError> {
           let device = if self.rnn.use_gpu { Device::GPU } else { Device::CPU };
           
           let deterministic = Tensor::zeros(&[batch_size, self.hidden_dim], device);
           let stochastic = Tensor::zeros(&[batch_size, self.discrete_dim * self.discrete_classes], device);
           let logits = Tensor::zeros(&[batch_size, self.discrete_dim * self.discrete_classes], device);

           Ok(RSSMState {
               deterministic,
               stochastic,
               logits,
           })
       }

       pub fn observe(&self, encoded_obs: Tensor, actions: Tensor) -> Result<RSSMState, MLError> {
           let batch_size = encoded_obs.shape()[0];
           let sequence_length = encoded_obs.shape()[1];
           
           let mut states = Vec::new();
           let mut current_state = self.initial_state(batch_size)?;
           
           for t in 0..sequence_length {
               let obs_t = encoded_obs.select(1, t)?;
               let action_t = actions.select(1, t)?;
               
               // Prior step: predict next state from current deterministic state
               let prior_logits = self.discrete_prior.forward(&current_state.deterministic)?;
               
               // Posterior step: incorporate observation
               let posterior_input = Tensor::cat(&[&current_state.deterministic, &obs_t], 1)?;
               let posterior_logits = self.discrete_posterior.forward(&posterior_input)?;
               
               // Sample stochastic state
               let stochastic = sample_categorical(&posterior_logits, self.discrete_dim, self.discrete_classes)?;
               
               // Update deterministic state with RNN
               let rnn_input = Tensor::cat(&[&stochastic, &action_t], 1)?;
               let (new_deterministic, _) = self.rnn.forward(&rnn_input.unsqueeze(1), Some((&current_state.deterministic, &Tensor::zeros_like(&current_state.deterministic))))?;
               let new_deterministic = new_deterministic.squeeze(1)?;
               
               current_state = RSSMState {
                   deterministic: new_deterministic,
                   stochastic: stochastic.clone(),
                   logits: posterior_logits,
               };
               
               states.push(current_state.clone());
           }
           
           // Stack all states into batch tensor
           self.stack_states(states)
       }

       pub fn imagine(&self, initial_state: &RSSMState, actions: &Tensor) -> Result<RSSMState, MLError> {
           let batch_size = actions.shape()[0];
           let sequence_length = actions.shape()[1];
           
           let mut states = Vec::new();
           let mut current_state = initial_state.clone();
           
           for t in 0..sequence_length {
               let action_t = actions.select(1, t)?;
               
               // Prior step only (no observation)
               let prior_logits = self.discrete_prior.forward(&current_state.deterministic)?;
               let stochastic = sample_categorical(&prior_logits, self.discrete_dim, self.discrete_classes)?;
               
               // Update deterministic state
               let rnn_input = Tensor::cat(&[&stochastic, &action_t], 1)?;
               let (new_deterministic, _) = self.rnn.forward(&rnn_input.unsqueeze(1), Some((&current_state.deterministic, &Tensor::zeros_like(&current_state.deterministic))))?;
               let new_deterministic = new_deterministic.squeeze(1)?;
               
               current_state = RSSMState {
                   deterministic: new_deterministic,
                   stochastic: stochastic.clone(),
                   logits: prior_logits,
               };
               
               states.push(current_state.clone());
           }
           
           self.stack_states(states)
       }

       pub fn kl_loss(&self, states: &RSSMState) -> Result<Tensor, MLError> {
           // Compute KL divergence between posterior and prior
           // This regularizes the latent space and prevents overfitting
           
           // For categorical distributions: KL(posterior || prior)
           let posterior_probs = softmax(&states.logits, -1)?;
           let prior_logits = self.discrete_prior.forward(&states.deterministic)?;
           let prior_probs = softmax(&prior_logits, -1)?;
           
           let kl = categorical_kl_divergence(&posterior_probs, &prior_probs)?;
           Ok(kl.mean())
       }
   }
   ```

3. **Observation Encoder** in `src/world_model/encoder.rs`:

   ```rust
   #[derive(Debug)]
   pub struct ObservationEncoder {
       pub conv_layers: Vec<Conv2D>,
       pub dense_layers: Vec<Dense>,
       pub layer_norm: LayerNorm,
       pub output_dim: usize,
   }

   impl ObservationEncoder {
       pub fn new(input_shape: &[usize], output_dim: usize, use_gpu: bool) -> Result<Self, MLError> {
           // Assume input is [channels, height, width]
           let mut conv_layers = Vec::new();
           
           // Convolutional feature extraction
           conv_layers.push(Conv2D::new(input_shape[0], 32, [4, 4], [2, 2], [1, 1], use_gpu)?);
           conv_layers.push(Conv2D::new(32, 64, [4, 4], [2, 2], [1, 1], use_gpu)?);
           conv_layers.push(Conv2D::new(64, 128, [4, 4], [2, 2], [1, 1], use_gpu)?);
           conv_layers.push(Conv2D::new(128, 256, [4, 4], [2, 2], [1, 1], use_gpu)?);
           
           // Calculate flattened size after convolutions
           let flattened_size = calculate_conv_output_size(input_shape, &conv_layers);
           
           // Dense layers for final encoding
           let mut dense_layers = Vec::new();
           dense_layers.push(Dense::new(flattened_size, 512, use_gpu)?);
           dense_layers.push(Dense::new(512, output_dim, use_gpu)?);
           
           let layer_norm = LayerNorm::new(output_dim, use_gpu)?;

           Ok(Self {
               conv_layers,
               dense_layers,
               layer_norm,
               output_dim,
           })
       }

       pub fn forward(&self, observations: &Tensor) -> Result<Tensor, MLError> {
           let mut x = observations.clone();
           
           // Apply convolutional layers with ReLU
           for conv in &self.conv_layers {
               x = conv.forward(&x)?;
               x = x.relu()?;
           }
           
           // Flatten for dense layers
           let batch_size = x.shape()[0];
           let sequence_length = x.shape()[1];
           let flattened_size = x.numel() / (batch_size * sequence_length);
           x = x.reshape(&[batch_size, sequence_length, flattened_size])?;
           
           // Apply dense layers
           for dense in &self.dense_layers {
               x = dense.forward(&x)?;
               if dense != self.dense_layers.last().unwrap() {
                   x = x.relu()?;
               }
           }
           
           // Layer normalization
           x = self.layer_norm.forward(&x)?;
           
           Ok(x)
       }
   }
   ```

4. **Observation Decoder** in `src/world_model/decoder.rs`:

   ```rust
   #[derive(Debug)]
   pub struct ObservationDecoder {
       pub dense_layers: Vec<Dense>,
       pub deconv_layers: Vec<ConvTranspose2D>,
       pub output_shape: Vec<usize>,
   }

   impl ObservationDecoder {
       pub fn new(input_dim: usize, output_shape: &[usize], use_gpu: bool) -> Result<Self, MLError> {
           let mut dense_layers = Vec::new();
           
           // Calculate initial dense layer size
           let initial_spatial_size = 4; // Start with 4x4 spatial dimension
           let initial_channels = 256;
           let dense_output_size = initial_channels * initial_spatial_size * initial_spatial_size;
           
           dense_layers.push(Dense::new(input_dim, 512, use_gpu)?);
           dense_layers.push(Dense::new(512, dense_output_size, use_gpu)?);
           
           // Transposed convolutions for upsampling
           let mut deconv_layers = Vec::new();
           deconv_layers.push(ConvTranspose2D::new(256, 128, [4, 4], [2, 2], [1, 1], use_gpu)?);
           deconv_layers.push(ConvTranspose2D::new(128, 64, [4, 4], [2, 2], [1, 1], use_gpu)?);
           deconv_layers.push(ConvTranspose2D::new(64, 32, [4, 4], [2, 2], [1, 1], use_gpu)?);
           deconv_layers.push(ConvTranspose2D::new(32, output_shape[0], [4, 4], [2, 2], [1, 1], use_gpu)?);

           Ok(Self {
               dense_layers,
               deconv_layers,
               output_shape: output_shape.to_vec(),
           })
       }

       pub fn forward(&self, latent_states: &Tensor) -> Result<Tensor, MLError> {
           let mut x = latent_states.clone();
           
           // Dense layers
           for dense in &self.dense_layers {
               x = dense.forward(&x)?;
               if dense != self.dense_layers.last().unwrap() {
                   x = x.relu()?;
               }
           }
           
           // Reshape to spatial format
           let batch_size = x.shape()[0];
           let sequence_length = x.shape()[1];
           x = x.reshape(&[batch_size, sequence_length, 256, 4, 4])?;
           
           // Transposed convolutions
           for (i, deconv) in self.deconv_layers.iter().enumerate() {
               x = deconv.forward(&x)?;
               if i < self.deconv_layers.len() - 1 {
                   x = x.relu()?;
               } else {
                   x = x.sigmoid()?; // Output in [0, 1] range
               }
           }
           
           Ok(x)
       }
   }
   ```

5. **Reward and Continue Predictors**:

   ```rust
   #[derive(Debug)]
   pub struct RewardPredictor {
       pub layers: Vec<Dense>,
   }

   impl RewardPredictor {
       pub fn new(input_dim: usize, use_gpu: bool) -> Result<Self, MLError> {
           let mut layers = Vec::new();
           layers.push(Dense::new(input_dim, 512, use_gpu)?);
           layers.push(Dense::new(512, 256, use_gpu)?);
           layers.push(Dense::new(256, 1, use_gpu)?);

           Ok(Self { layers })
       }

       pub fn forward(&self, states: &Tensor) -> Result<Tensor, MLError> {
           let mut x = states.clone();
           for (i, layer) in self.layers.iter().enumerate() {
               x = layer.forward(&x)?;
               if i < self.layers.len() - 1 {
                   x = x.relu()?;
               }
           }
           Ok(x)
       }
   }

   #[derive(Debug)]
   pub struct ContinuePredictor {
       pub layers: Vec<Dense>,
   }

   impl ContinuePredictor {
       pub fn new(input_dim: usize, use_gpu: bool) -> Result<Self, MLError> {
           let mut layers = Vec::new();
           layers.push(Dense::new(input_dim, 512, use_gpu)?);
           layers.push(Dense::new(512, 256, use_gpu)?);
           layers.push(Dense::new(256, 1, use_gpu)?);

           Ok(Self { layers })
       }

       pub fn forward(&self, states: &Tensor) -> Result<Tensor, MLError> {
           let mut x = states.clone();
           for (i, layer) in self.layers.iter().enumerate() {
               x = layer.forward(&x)?;
               if i < self.layers.len() - 1 {
                   x = x.relu()?;
               } else {
                   x = x.sigmoid()?; // Probability of episode continuing
               }
           }
           Ok(x)
       }
   }
   ```

6. **Physics Integration** in `src/world_model/physics_env.rs`:

   ```rust
   use crate::env::Env;
   use physics::PhysicsSim;

   pub struct PhysicsWorldModelEnv {
       physics_sim: PhysicsSim,
       world_model: DreamerV3,
       observation_buffer: Vec<Tensor>,
       action_buffer: Vec<Tensor>,
       use_imagination: bool,
   }

   impl PhysicsWorldModelEnv {
       pub fn new(physics_sim: PhysicsSim, world_model: DreamerV3) -> Self {
           Self {
               physics_sim,
               world_model,
               observation_buffer: Vec::new(),
               action_buffer: Vec::new(),
               use_imagination: false,
           }
       }

       pub fn switch_to_imagination(&mut self, initial_state: RSSMState) {
           self.use_imagination = true;
           // Set up imagination mode with initial state
       }

       pub fn extract_observation(&self) -> Tensor {
           // Extract observation from physics simulation
           // Convert to tensor format expected by world model
           let sphere_positions = self.physics_sim.spheres.iter()
               .map(|s| [s.pos.x, s.pos.y, s.pos.z])
               .collect::<Vec<_>>();
           
           Tensor::from_slice(&sphere_positions.concat(), &[1, sphere_positions.len() * 3])
       }
   }

   impl Env for PhysicsWorldModelEnv {
       fn step(&mut self, action: &Tensor) -> Result<(Tensor, f32, bool), EnvError> {
           if self.use_imagination {
               // Use world model for imagination
               let imagination = self.world_model.imagine(&self.current_state, &action.unsqueeze(0))?;
               let obs = imagination.observations.squeeze(0)?;
               let reward = imagination.rewards.squeeze(0)?.item::<f32>();
               let done = imagination.continues.squeeze(0)?.item::<f32>() < 0.5;
               Ok((obs, reward, done))
           } else {
               // Use real physics simulation
               self.apply_action_to_physics(action)?;
               self.physics_sim.step_cpu();
               
               let obs = self.extract_observation();
               let reward = self.compute_reward();
               let done = self.check_termination();
               
               // Store for world model training
               self.observation_buffer.push(obs.clone());
               self.action_buffer.push(action.clone());
               
               Ok((obs, reward, done))
           }
       }

       fn reset(&mut self) -> Result<Tensor, EnvError> {
           self.physics_sim = PhysicsSim::new(); // Reset physics
           self.use_imagination = false;
           self.observation_buffer.clear();
           self.action_buffer.clear();
           Ok(self.extract_observation())
       }
   }
   ```

7. **Comprehensive Testing** in `tests/dreamer_v3.rs`:

   ```rust
   #[test]
   fn test_rssm_forward_pass() {
       let config = DreamerV3Config {
           latent_dim: 32,
           hidden_dim: 64,
           discrete_dim: 8,
           discrete_classes: 8,
           sequence_length: 10,
           ..Default::default()
       };
       
       let rssm = RecurrentStateSpaceModel::new(&config).unwrap();
       
       let batch_size = 4;
       let sequence_length = 10;
       let encoded_obs = Tensor::randn(&[batch_size, sequence_length, config.latent_dim]);
       let actions = Tensor::randn(&[batch_size, sequence_length, 4]);
       
       let states = rssm.observe(encoded_obs, actions).unwrap();
       
       assert_eq!(states.deterministic.shape(), &[batch_size, sequence_length, config.hidden_dim]);
       assert_eq!(states.stochastic.shape(), &[batch_size, sequence_length, config.discrete_dim * config.discrete_classes]);
   }

   #[test]
   fn test_world_model_reconstruction() {
       let observation_shape = [3, 64, 64]; // RGB 64x64
       let config = DreamerV3Config::default();
       
       let world_model = DreamerV3::new(config, &observation_shape).unwrap();
       
       let batch_size = 2;
       let sequence_length = 5;
       let observations = Tensor::randn(&[batch_size, sequence_length, 3, 64, 64]);
       let actions = Tensor::randn(&[batch_size, sequence_length, 4]);
       
       let states = world_model.encode_sequence(&observations, &actions).unwrap();
       let reconstructed = world_model.decoder.forward(&states.features()).unwrap();
       
       assert_eq!(reconstructed.shape(), observations.shape());
   }

   #[test]
   fn test_imagination_rollout() {
       let config = DreamerV3Config::default();
       let world_model = DreamerV3::new(config, &[3, 64, 64]).unwrap();
       
       let batch_size = 2;
       let rollout_length = 15;
       let initial_state = world_model.rssm.initial_state(batch_size).unwrap();
       let actions = Tensor::randn(&[batch_size, rollout_length, 4]);
       
       let imagination = world_model.imagine(&initial_state, &actions).unwrap();
       
       assert_eq!(imagination.states.deterministic.shape()[1], rollout_length);
       assert_eq!(imagination.rewards.shape(), &[batch_size, rollout_length, 1]);
   }

   #[test]
   fn test_kl_loss_computation() {
       let config = DreamerV3Config::default();
       let rssm = RecurrentStateSpaceModel::new(&config).unwrap();
       
       let batch_size = 4;
       let sequence_length = 10;
       let state = rssm.initial_state(batch_size).unwrap();
       
       let kl_loss = rssm.kl_loss(&state).unwrap();
       
       assert_eq!(kl_loss.shape(), &[]);
       assert!(kl_loss.item::<f32>() >= 0.0); // KL divergence is non-negative
   }

   #[test]
   fn test_physics_integration() {
       let mut physics_sim = PhysicsSim::new();
       physics_sim.add_sphere(Vec3::new(0.0, 5.0, 0.0), Vec3::ZERO, 1.0);
       physics_sim.add_plane(Vec3::new(0.0, 1.0, 0.0), 0.0);
       
       let config = DreamerV3Config::default();
       let world_model = DreamerV3::new(config, &[3]).unwrap(); // Simple 3D position observation
       
       let mut env = PhysicsWorldModelEnv::new(physics_sim, world_model);
       
       // Collect real experience
       let mut real_observations = Vec::new();
       let mut real_actions = Vec::new();
       
       for _ in 0..50 {
           let action = Tensor::randn(&[3]); // 3D force
           let (obs, _reward, _done) = env.step(&action).unwrap();
           real_observations.push(obs);
           real_actions.push(action);
       }
       
       // Train world model on collected data
       let batch = WorldModelBatch::new(real_observations, real_actions);
       let loss = env.world_model.compute_loss(&batch).unwrap();
       
       assert!(loss.total.item::<f32>() > 0.0);
   }
   ```

### Success Criteria

- RSSM successfully learns temporal dynamics in latent space
- World model can accurately reconstruct physics simulation observations
- Imagination rollouts generate plausible future states
- KL regularization prevents posterior collapse while maintaining expressiveness
- Integration with physics environments works seamlessly
- Model-based policy optimization shows improvement over model-free methods
- Latent representations capture meaningful behavioral features for novelty search
- GPU acceleration provides significant training speedup
- All tests pass with comprehensive coverage

### Integration Notes

- Depends on Task 2A for efficient GPU tensor operations
- Latent representations will be used in Task 4B for novelty search
- World model enables sample-efficient learning for creature evolution
- Consider memory requirements for long sequence modeling
- Document hyperparameter sensitivity and tuning guidelines